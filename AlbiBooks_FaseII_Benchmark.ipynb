```markdown
# AlbiBooks Fase II - Notebook de Benchmarking Automatizado

## Objetivo
Este notebook é projetado para realizar um benchmark automatizado do sistema AlbiBooks Fase II, que utiliza a API Mistral para embeddings e a API Gemini para geração de respostas. O objetivo é processar um conjunto predefinido de perguntas, recolher respostas, documentos recuperados, contexto enviado ao LLM, e métricas de desempenho (tempo de execução e uso de memória) para cada pergunta.

## Pré-requisitos
1.  **Ficheiros de Índice:** Os artefactos de indexação gerados pelo `Copy_of_GrokTryIndexacao.ipynb` (ou similar) devem estar presentes no diretório especificado pela variável `EMBEDDINGS_DIR`. Estes incluem:
    *   `book_embeddings.npy`
    *   `book_metadata.pkl`
    *   `book_index.faiss`
2.  **Ficheiro de Perguntas:** Um ficheiro JSON (por defeito, `benchmark_universitario_perguntas.json`) contendo a lista de perguntas para o benchmark. Cada pergunta deve ter um "id" e um campo "pergunta".
3.  **API Keys:** As chaves API para Mistral AI e Google AI Studio (Gemini) devem estar configuradas como segredos no ambiente Google Colab (`MISTRAL_API_KEY` e `GEMINI_API_KEY` - note que os nomes foram padronizados para `MISTRAL_API_KEY` e `GEMINI_API_KEY` no código).
4.  **Bibliotecas Python:** Todas as bibliotecas listadas na primeira célula de código devem estar instaladas.

## Funcionalidades Principais
- **Modo de Simulação:**
    - Uma flag `MODO_SIMULACAO` (booleana) permite executar o notebook sem fazer chamadas reais às APIs. Quando `True`, utiliza dados "dummy" para embeddings, busca, contexto e respostas. Isto é útil para testar o fluxo do notebook rapidamente.
    - `NUM_PERGUNTAS_SIMULACAO` limita o número de perguntas processadas em modo de simulação.
    - **IMPORTANTE:** Para uma execução real do benchmark, `MODO_SIMULACAO` deve ser definida como `False`.
- **Carregamento de Perguntas:** Lê as perguntas do ficheiro JSON especificado.
- **Processamento RAG por Pergunta:** Para cada pergunta:
    - Inicializa um histórico de conversa vazio (para independência dos testes).
    - Chama a função `answer_question` que:
        - Gera embedding da pergunta (Mistral API, ou simulado).
        - Realiza busca no índice FAISS (ou simulado).
        - Formata o contexto com os documentos recuperados.
        - Gera uma resposta usando Gemini API (ou simulado).
        - Mede o tempo de execução e o uso de memória.
- **Recolha de Resultados Detalhados:** Para cada pergunta, armazena:
    - ID da pergunta.
    - Texto da pergunta.
    - Resposta gerada.
    - Documentos recuperados (com detalhes como título, autor, score de relevância).
    - Contexto exato enviado ao Gemini.
    - Métricas de desempenho (tempo, memória).
- **Armazenamento dos Resultados:** Guarda todos os resultados num ficheiro JSON Lines (e.g., `benchmark_fase_ii_resultados.jsonl`), onde cada linha é um objeto JSON representando os resultados de uma pergunta.

## Como Utilizar
1.  Assegure-se de que todos os pré-requisitos estão cumpridos (ficheiros de índice, ficheiro de perguntas, API keys).
2.  Configure a flag `MODO_SIMULACAO`:
    - `True` para testes rápidos de fluxo.
    - `False` para a execução real do benchmark com chamadas às APIs.
3.  Defina os caminhos para o `perguntas_file` e `resultados_file` na última célula.
4.  Execute todas as células do notebook.
5.  Os resultados serão guardados no `resultados_file` especificado.

```python
# AlbiBooks Fase II - Notebook de Benchmark
# Este notebook é uma adaptação do Copy_of_GeminiFlash2p5V6Stable.ipynb
# para executar benchmarks automatizados com um conjunto de perguntas.

# 1. Instalação de Bibliotecas
!pip install -q google-generativeai faiss-cpu requests tqdm

# 2. Importações de Bibliotecas
import json
import os
import time
import numpy as np
import pickle
import requests
from tqdm.auto import tqdm
import google.generativeai as genai
from google.colab import userdata
import tracemalloc # Para medição de memória
import threading # Para timeouts

# 3. Configurações Globais

# Configuração para Modo de Simulação
MODO_SIMULACAO = True
NUM_PERGUNTAS_SIMULACAO = 5 # Processar apenas N perguntas em modo de simulação


# Obtenção das API Keys do Colab Userdata
try:
    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')
    MISTRAL_API_KEY = userdata.get('MISTRAL_API_KEY')
except Exception as e:
    print(f"Erro ao obter API keys do Colab Userdata: {e}")
    if not MODO_SIMULACAO:
        print("AVISO: API keys não encontradas. A execução real falhará.")
    else:
        print("INFO: API keys não encontradas, mas MODO_SIMULACAO está ativo.")
    GEMINI_API_KEY = None
    MISTRAL_API_KEY = None

if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

# Diretório para os embeddings e metadados no Google Drive
try:
    from google.colab import drive
    drive.mount('/content/drive')
    EMBEDDINGS_DIR = '/content/drive/MyDrive/AlbiBooksIndex/' # Adapte este caminho
    if not os.path.exists(EMBEDDINGS_DIR):
        os.makedirs(EMBEDDINGS_DIR, exist_ok=True)
        print(f"Directório {EMBEDDINGS_DIR} criado/verificado.")
except Exception as e:
    print(f"Erro ao montar o Google Drive ou configurar EMBEDDINGS_DIR: {e}")
    EMBEDDINGS_DIR = './AlbiBooksIndex/'
    if not os.path.exists(EMBEDDINGS_DIR):
        os.makedirs(EMBEDDINGS_DIR, exist_ok=True)
    print(f"Usando directório local de fallback: {EMBEDDINGS_DIR}")


# Configurações da API Mistral
MISTRAL_API_URL = "https://api.mistral.ai/v1/embeddings"
MISTRAL_MODEL_NAME = "mistral-embed"
EMBEDDING_DIM = 1024

# Configurações para chamadas à API Mistral (retry)
BASE_WAIT_TIME = 1.5
MAX_RETRIES_MISTRAL = 5

# Configurações para chamadas à API Gemini
GEMINI_MODEL_NAME = 'gemini-1.5-flash-latest'
GEMINI_TIMEOUT = 45
GEMINI_MAX_RETRIES = 3
DEFAULT_GEMINI_GENERATION_CONFIG = {
    "temperature": 0.7, "top_p": 0.95, "top_k": 40,
}
DEFAULT_GEMINI_SAFETY_SETTINGS = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
]

TOP_K = 5
MAX_HISTORY_LENGTH = 0

# 4. Função Decoradora @measure_execution
def measure_execution(func):
    def wrapper(*args, **kwargs):
        tracemalloc.start()
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        metrics = {
            "execution_time_seconds": round(end_time - start_time, 3),
            "memory_used_MB": round(current / 10**6, 3),
            "memory_peak_MB": round(peak / 10**6, 3)
        }
        if isinstance(result, tuple):
            return (*result, metrics)
        else:
            return result, metrics
    return wrapper

# 5. Funções Utilitárias e Principais

def initialize_retriever(embeddings_dir: str):
    if MODO_SIMULACAO:
        print("MODO_SIMULACAO: Inicialização do retriever real ignorada.")
        # Retornar estruturas dummy compatíveis para evitar erros
        dummy_embeddings = np.random.rand(2, EMBEDDING_DIM).astype('float32')
        dummy_metadata = [
            {'id': 'sim_id_1', 'title': 'Livro Simulado 1', 'authors': ['Autor Simulado A'], 'language_code': 'pt', 'subjects_normalized_short_display': ['Simulação'], 'shelvingloc': 'SIM PRT', 'call_no': 'SIM 001', 'itype': 'LIVRO'},
            {'id': 'sim_id_2', 'title': 'Outro Livro Simulado', 'authors': ['Autor Simulado B'], 'language_code': 'en', 'subjects_normalized_short_display': ['Teste', 'Simulação'], 'shelvingloc': 'SIM PRT', 'call_no': 'SIM 002', 'itype': 'LIVRO'}
        ]
        # Criar um índice FAISS dummy simples
        import faiss
        dummy_index = faiss.IndexFlatL2(EMBEDDING_DIM)
        if dummy_embeddings.shape[0] > 0: # FAISS precisa de pelo menos um vetor para adicionar
             dummy_index.add(dummy_embeddings)
        print(f"Retriever simulado inicializado com {dummy_index.ntotal} itens.")
        return dummy_embeddings, dummy_metadata, dummy_index

    embeddings_path = os.path.join(embeddings_dir, 'book_embeddings.npy')
    metadata_path = os.path.join(embeddings_dir, 'book_metadata.pkl')
    index_path = os.path.join(embeddings_dir, 'book_index.faiss')
    if not all(os.path.exists(p) for p in [embeddings_path, metadata_path, index_path]):
        print(f"ERRO: Ficheiros de índice não encontrados em {embeddings_dir}.")
        return None, None, None
    try:
        embeddings = np.load(embeddings_path)
        with open(metadata_path, 'rb') as f: metadata = pickle.load(f)
        import faiss
        index = faiss.read_index(index_path)
        print(f"Retriever real carregado: {index.ntotal} itens.")
        return embeddings, metadata, index
    except Exception as e:
        print(f"Erro ao carregar dados do retriever real: {e}")
        return None, None, None

def get_embedding(text: str, retries: int = MAX_RETRIES_MISTRAL, wait_time: float = BASE_WAIT_TIME) -> list[float] | None:
    if MODO_SIMULACAO:
        return np.random.rand(EMBEDDING_DIM).astype('float32').tolist()

    if not MISTRAL_API_KEY: return None
    headers = {"Authorization": f"Bearer {MISTRAL_API_KEY}", "Content-Type": "application/json", "Accept": "application/json"}
    data = {"model": MISTRAL_MODEL_NAME, "input": [text]}
    for attempt in range(retries):
        try:
            response = requests.post(MISTRAL_API_URL, headers=headers, json=data, timeout=20)
            response.raise_for_status()
            return response.json()["data"][0]["embedding"]
        except Exception as e:
            if attempt == retries - 1: return None
            time.sleep(wait_time * (2**attempt))
    return None

def search(query_embedding: np.ndarray, index: 'faiss.Index', embeddings: np.ndarray, metadata: list, top_k: int = TOP_K) -> list[dict]:
    # A lógica de simulação principal está em answer_question.
    # Esta função será chamada por answer_question. Se MODO_SIMULACAO for True,
    # answer_question já terá retornado dados simulados antes de chegar aqui no fluxo não simulado.
    # No entanto, se initialize_retriever retornar um index dummy, esta função precisa lidar com isso.
    if query_embedding is None or index is None or index.ntotal == 0: # Adicionado index.ntotal == 0 para o caso de indice dummy vazio
        return []

    query_embedding_2d = np.array([query_embedding], dtype=np.float32)
    distances, indices = index.search(query_embedding_2d, min(top_k, index.ntotal)) # Garantir que top_k não é maior que ntotal
    results = []
    for i, idx in enumerate(indices[0]):
        if idx == -1: continue
        relevance_score = 1.0 / (1.0 + distances[0][i])
        # Certificar que metadata[idx] é seguro, especialmente se o índice for dummy
        if idx < len(metadata):
            book_meta = metadata[idx]
            results.append({
                "id": book_meta.get("id", str(idx)), "titulo": book_meta.get("title", "N/A"),
                "autor": book_meta.get("authors", "N/A"), "relevance_score": round(relevance_score, 4),
                "index_original": int(idx)
            })
        else: # Caso de segurança para índice dummy e metadata desalinhados
            results.append({
                "id": str(idx), "titulo": "Metadata Dummy Desalinhada", "autor": "N/A",
                "relevance_score": round(relevance_score, 4), "index_original": int(idx)
            })
    return results

def format_context(search_results: list[dict], metadata: list) -> str:
    # A lógica de simulação principal está em answer_question.
    if not search_results: return "Nenhum documento relevante encontrado."
    context_parts = ["Contexto de documentos relevantes:\n"]
    for i, result in enumerate(search_results):
        # Se metadata for a dummy de initialize_retriever e search_results também for dummy,
        # o index_original pode não corresponder. A simulação em answer_question deve gerar search_results
        # cujos index_original sejam válidos para a metadata que ela própria usa.
        if result['index_original'] < len(metadata):
            book_meta = metadata[result['index_original']]
        else: # Fallback se index_original estiver fora dos limites da metadata (e.g. simulação mista)
            book_meta = {'title': result.get('titulo', 'Título Desconhecido'), 'authors': [result.get('autor', 'Autor Desconhecido')]}

        context_parts.append(f"Documento {i+1} (ID: {result.get('id', 'N/A')}, Relevância: {result.get('relevance_score', 'N/A')}):")
        context_parts.append(f"  Título: {book_meta.get('title', 'N/A')}")
        context_parts.append(f"  Autor(es): {', '.join(book_meta.get('authors', [])) if book_meta.get('authors') else 'N/A'}")
        if 'co-author' in book_meta and book_meta['co-author']: context_parts.append(f"  Co-autor(es): {', '.join(book_meta['co-author'])}")
        context_parts.append(f"  Idioma: {book_meta.get('language_code', 'N/A')}")
        context_parts.append(f"  Assuntos: {', '.join(book_meta.get('subjects_normalized_short_display', [])) if book_meta.get('subjects_normalized_short_display') else 'N/A'}")
        context_parts.append(f"  Localização: {book_meta.get('shelvingloc', 'N/A')}")
        context_parts.append(f"  Cota: {book_meta.get('call_no', 'N/A')}")
        context_parts.append(f"  Tipo de Item: {book_meta.get('itype', 'N/A')}")
        context_parts.append("-" * 20)
    return "\n".join(context_parts)

def format_history(history: list[tuple[str, str]]) -> str:
    if not history: return ""
    formatted_history = ["Histórico da Conversa Anterior:"]
    for user_msg, model_msg in history:
        formatted_history.append(f"Utilizador: {user_msg}")
        formatted_history.append(f"Assistente: {model_msg}")
    return "\n".join(formatted_history) + "\n"


def generate_gemini_response(prompt: str, retries: int = GEMINI_MAX_RETRIES, timeout: int = GEMINI_TIMEOUT) -> str | None:
    # A simulação principal está em answer_question
    if not GEMINI_API_KEY and not MODO_SIMULACAO : # Adicionado not MODO_SIMULACAO
        print("GEMINI_API_KEY não configurada para modo real.")
        return "ERROR_API_KEY_MISSING"

    if MODO_SIMULACAO: # Esta verificação é redundante se answer_question já simula.
         return f"Resposta simulada do Gemini para o prompt que começa com: '{prompt[:50]}...'"

    model = genai.GenerativeModel(model_name=GEMINI_MODEL_NAME, generation_config=DEFAULT_GEMINI_GENERATION_CONFIG, safety_settings=DEFAULT_GEMINI_SAFETY_SETTINGS)
    response_container = {"response": None, "error": None}
    def target():
        try:
            gemini_response = model.generate_content(prompt)
            response_container["response"] = gemini_response.text
        except Exception as e:
            response_container["error"] = e
    for attempt in range(retries):
        thread = threading.Thread(target=target)
        thread.start(); thread.join(timeout=timeout)
        if thread.is_alive():
            if attempt == retries - 1: return "TIMEOUT_ERROR"
            time.sleep(BASE_WAIT_TIME) # Não usar exponential backoff aqui, apenas esperar um pouco.
            continue
        if response_container["error"]:
            if attempt == retries - 1: return f"API_ERROR: {str(response_container['error'])}"
            time.sleep(BASE_WAIT_TIME * (2**attempt)) # Exponential backoff para erros da API
            response_container = {"response": None, "error": None}
            continue
        return response_container["response"]
    return "ERROR_MAX_RETRIES" # Fallback

@measure_execution
def answer_question(query: str, embeddings: np.ndarray | None, metadata: list | None, index: 'faiss.Index' | None, history: list[tuple[str, str]]):
    if MODO_SIMULACAO:
        # print(f"MODO_SIMULACAO: answer_question para '{query}'")
        time.sleep(np.random.uniform(0.1, 0.3))

        # Usar os metadados dummy carregados por initialize_retriever em modo de simulação
        # para que os index_original façam sentido.
        # Se metadata for None (e.g. erro no init simulado), criar uma pequena lista local.
        if metadata is None or len(metadata) < 2:
            effective_metadata = [
                {'id': 'sim_meta_1', 'title': 'Livro Simulado Alpha', 'authors': ['Autor Simulado A.']},
                {'id': 'sim_meta_2', 'title': 'Manual Simulado Beta', 'authors': ['Coletivo Simulado B.']}
            ]
        else:
            effective_metadata = metadata

        sim_search_results = []
        num_sim_docs = min(2, len(effective_metadata)) # Garantir que não tentamos aceder a mais do que temos
        for i in range(num_sim_docs):
            sim_search_results.append({
                'id': effective_metadata[i].get('id', f'sim_doc_{i}'),
                'titulo': f"{effective_metadata[i].get('title','Título Sim')} para '{query.split(' ')[-1]}'", # Adicionar parte da query ao título
                'autor': effective_metadata[i].get('authors', ['Autor Sim.'])[0],
                'relevance_score': round(np.random.uniform(0.75,0.99),4),
                'index_original': i
            })

        sim_formatted_context = format_context(sim_search_results, effective_metadata)
        sim_response = f"Resposta simulada para '{query}'. Documentos importantes: '{sim_search_results[0]['titulo']}'."
        if len(sim_search_results) > 1:
             sim_response += f" E também '{sim_search_results[1]['titulo']}'."

        return sim_response, sim_search_results, sim_formatted_context

    # Lógica original (não será executada se MODO_SIMULACAO = True no início da função)
    print(f"\nProcessando pergunta (Real): \"{query}\"")
    print("A gerar embedding para a pergunta...")
    query_embedding_val = get_embedding(query) # Renomeado para evitar conflito com o módulo
    if query_embedding_val is None:
        error_msg = "Falha ao gerar embedding para a pergunta."
        print(error_msg); return error_msg, [], "N/A Context (Embedding Error)"

    print("A buscar documentos relevantes...")
    search_results_val = search(query_embedding_val, index, embeddings, metadata, top_k=TOP_K) # Renomeado
    if not search_results_val: print("Nenhum documento relevante encontrado.")
    else: print(f"Encontrados {len(search_results_val)} documentos relevantes.")

    print("A formatar contexto para o LLM...")
    formatted_context_val = format_context(search_results_val, metadata) # Renomeado
    formatted_history_val = format_history(history) # Renomeado

    prompt_instructions = (
        "Você é um assistente de biblioteca amigável. "
        "Responda à pergunta do utilizador com base no contexto de documentos relevantes fornecido e no histórico da conversa. "
        "Se não houver informação suficiente no contexto para responder à pergunta, seja honesto e diga que não encontrou a informação nos documentos. "
        "Se a pergunta for sobre a localização ou cota de um livro, e essa informação estiver no contexto, forneça-a claramente. "
        "Seja conciso quando possível, mas forneça os detalhes importantes."
    )
    full_prompt = f"{prompt_instructions}\n\n{formatted_history_val}Contexto:\n{formatted_context_val}\n\nPergunta do Utilizador: {query}\n\nResposta do Assistente:"

    print("A gerar resposta com o Gemini...")
    response_val = generate_gemini_response(full_prompt) # Renomeado
    if response_val is None or "ERROR" in response_val : response_val = "Falha ao gerar resposta do Gemini ou erro na API."

    print(f"Resposta gerada: \"{response_val}\"")
    return response_val, search_results_val, formatted_context_val


def run_benchmark(perguntas_filepath: str, resultados_filepath: str, embeddings_dir: str):
    print(f"A iniciar benchmark...")
    if MODO_SIMULACAO: print("### EXECUTANDO EM MODO DE SIMULAÇÃO ###")
    print(f"Ficheiro de perguntas: {perguntas_filepath}")
    print(f"Ficheiro de resultados: {resultados_filepath}")

    try:
        with open(perguntas_filepath, 'r', encoding='utf-8') as f: lista_perguntas_completa = json.load(f)
        processar_n_perguntas = NUM_PERGUNTAS_SIMULACAO if MODO_SIMULACAO else len(lista_perguntas_completa)
        lista_perguntas = lista_perguntas_completa[:processar_n_perguntas]
        print(f"Carregadas {len(lista_perguntas_completa)} perguntas. Processando {len(lista_perguntas)}.")
    except Exception as e:
        print(f"ERRO ao carregar/processar ficheiro de perguntas '{perguntas_filepath}': {e}"); return

    print("\nA inicializar o retriever...")
    embeddings_data, metadata_list, faiss_index = initialize_retriever(embeddings_dir)
    if not MODO_SIMULACAO and (embeddings_data is None or metadata_list is None or faiss_index is None):
        print("ERRO: Falha ao inicializar o retriever real. Benchmark abortado."); return
    # Em modo de simulação, initialize_retriever já imprime a sua mensagem e retorna dummies.
    print("Retriever inicializado (ou simulado).")

    resultados_detalhados = []
    for i, item_pergunta in enumerate(tqdm(lista_perguntas, desc="Processando Perguntas")):
        query = item_pergunta.get("pergunta")
        if not query:
            print(f"AVISO: Pergunta vazia no item {i}. A ignorar.")
            continue

        # A função answer_question já está decorada e agora retorna 4 itens
        # (response, search_results, formatted_context, metrics)
        # Nomes internos das variáveis shadowam os nomes das funções, o que é ok aqui.
        response, search_results, formatted_context, metrics = answer_question(
            query=query, embeddings=embeddings_data, metadata=metadata_list,
            index=faiss_index, history=[]
        )
        resultados_detalhados.append({
            "id_pergunta": item_pergunta.get('id', f"q_{i+1}"), "pergunta_texto": query,
            "resposta_gerada": response, "documentos_recuperados": search_results,
            "contexto_para_gemini": formatted_context, "metricas_desempenho": metrics
        })
    try:
        with open(resultados_filepath, 'w', encoding='utf-8') as f:
            for res_item in resultados_detalhados:
                f.write(json.dumps(res_item, ensure_ascii=False) + '\n')
        print(f"\nBenchmark concluído. {len(resultados_detalhados)} resultados guardados em '{resultados_filepath}'.")
    except IOError as e:
        print(f"ERRO: Falha ao escrever ficheiro de resultados em '{resultados_filepath}': {e}")

# 7. Célula para Executar o Benchmark e Verificar Saída Simulada

if __name__ == '__main__': # Ou executar diretamente numa célula de notebook

    print(f"MODO_SIMULACAO está definido como: {MODO_SIMULACAO}")

    perguntas_file = "benchmark_universitario_perguntas.json"
    if not os.path.exists(perguntas_file):
        print(f"AVISO: Ficheiro de perguntas '{perguntas_file}' não encontrado. A criar um de exemplo.")
        dummy_q_list = [{"id": f"dummy_q{i+1}", "pergunta": f"Pergunta simulada número {i+1} sobre tópico aleatório?"} for i in range(NUM_PERGUNTAS_SIMULACAO)]
        with open(perguntas_file, 'w', encoding='utf-8') as f_dummy:
            json.dump(dummy_q_list, f_dummy, ensure_ascii=False, indent=2)
        print(f"Ficheiro de exemplo '{perguntas_file}' criado com {NUM_PERGUNTAS_SIMULACAO} perguntas.")

    if not os.path.exists(EMBEDDINGS_DIR):
        os.makedirs(EMBEDDINGS_DIR, exist_ok=True)
        print(f"Diretório de resultados '{EMBEDDINGS_DIR}' criado.")

    simulated_results_file = os.path.join(EMBEDDINGS_DIR, "simulated_benchmark_fase_ii_resultados.jsonl")

    print(f"\nA executar run_benchmark para gerar: {simulated_results_file}")
    run_benchmark(perguntas_filepath=perguntas_file,
                  resultados_filepath=simulated_results_file,
                  embeddings_dir=EMBEDDINGS_DIR)

    print(f"\n--- Conteúdo do ficheiro de resultados simulados '{simulated_results_file}' (primeiras linhas) ---")
    try:
        with open(simulated_results_file, 'r', encoding='utf-8') as f_res:
            for i in range(NUM_PERGUNTAS_SIMULACAO + 1):
                line = f_res.readline()
                if not line: break
                print(line.strip())
    except FileNotFoundError:
        print(f"ERRO: Ficheiro de resultados simulados '{simulated_results_file}' não foi criado.")
    except Exception as e:
        print(f"Erro ao ler o ficheiro de resultados simulados: {e}")

```
