## Justificações Técnicas para as Principais Decisões Tomadas

A evolução do sistema de recomendação de livros, desde a versão inicial em `AlbiBooks.ipynb` até à versão final em `Copy_of_GeminiFlash2p5V6Stable.ipynb`, foi marcada por várias decisões chave de design e implementação. Estas decisões visaram otimizar o desempenho, melhorar a qualidade das respostas, aumentar a robustez e enriquecer a experiência do utilizador. Abaixo, apresentam-se as justificações técnicas para as principais alterações:

1.  **Mudança de LLMs Locais para LLMs via API (Google Gemini):**
    *   **Justificação Técnica:** A migração de modelos de linguagem (LLMs) executados localmente (como Zephyr 7B Beta ou Llama 2 13B quantizados) para um modelo avançado via API (especificamente `gemini-2.5-flash-preview-05-20`) foi motivada pela busca por maior capacidade de geração de texto, melhor compreensão de nuances e seguimento de instruções complexas. Modelos de grande escala como os da família Gemini, mantidos e atualizados por provedores de API, geralmente superam modelos menores executáveis localmente, mesmo que estes últimos sejam quantizados. Esta abordagem também elimina a necessidade de gestão local de hardware robusto (GPUs com alta VRAM) e as complexidades associadas à configuração e manutenção desses modelos. Como resultado direto, observou-se uma redução significativa na carga computacional da máquina cliente, refletida em tempos de resposta mais rápidos para o utilizador final (2-10s na API vs. 15-90s localmente) e um consumo de memória local drasticamente menor (pico de ~0.3MB vs. ~2MB).

2.  **Uso da API da Mistral para Embeddings de Query (`mistral-embed`):**
    *   **Justificação Técnica:** A decisão de utilizar a API `mistral-embed` para gerar embeddings das perguntas dos utilizadores, em substituição ao modelo local `thenlper/gte-small`, baseou-se na premissa de que modelos de embedding especializados e servidos por API podem oferecer uma representação semântica mais precisa e rica das queries. Uma melhor qualidade no embedding da query leva a uma recuperação de informação mais relevante do índice FAISS, impactando positivamente a qualidade do contexto fornecido ao LLM e, por conseguinte, a precisão da resposta final. Adicionalmente, elimina a necessidade de carregar um modelo de embedding na memória local apenas para processar as queries, otimizando recursos.

3.  **Utilização de Embeddings de Documentos e Índice FAISS Pré-calculados:**
    *   **Justificação Técnica:** O sistema final passou a carregar embeddings de documentos e um índice FAISS que foram gerados offline. Com um corpus significativamente maior (18.266 livros na versão final contra 500 na original), gerar embeddings para todos os documentos e construir o índice FAISS a cada execução seria computacionalmente proibitivo e resultaria em tempos de inicialização excessivamente longos. A pré-calculação destes recursos é uma otimização crucial que torna o arranque do sistema quase instantâneo e viabiliza a sua operação com grandes volumes de dados. Esta abordagem também simplifica a atualização da base de conhecimento, que pode ser feita como um processo batch separado.

4.  **Aumento do Volume e Melhoria da Qualidade dos Dados de Conhecimento:**
    *   **Justificação Técnica:** A expansão da base de conhecimento de 500 descrições genéricas do Goodreads para 18.266 registos de livros com metadados detalhados (incluindo campos como `shelvingloc` para localização física, `call_no` para cota, e `itype` para tipo de item) enriquece profundamente o contexto disponível para o sistema RAG. Uma base de dados maior e com informações mais específicas de um catálogo de biblioteca permite ao assistente fornecer respostas mais precisas, informativas e diretamente úteis para um utilizador que procura livros numa biblioteca específica, em vez de recomendações genéricas.

5.  **Implementação de Gestão de Histórico de Conversa:**
    *   **Justificação Técnica:** A introdução da gestão de histórico de conversa (limitado às últimas `MAX_HISTORY_LENGTH` interações) é uma melhoria fundamental para a experiência conversacional (UX). Ao incluir o histórico formatado no prompt enviado ao LLM (Gemini), o sistema ganha a capacidade de manter o contexto ao longo de múltiplas trocas. Isto permite que o LLM compreenda referências a entidades ou tópicos mencionados anteriormente e responda a perguntas de seguimento de forma mais natural e coesa, aproximando a interação de um diálogo humano e aumentando a sua utilidade.

6.  **Adoção de uma Interface Gráfica (Gradio):**
    *   **Justificação Técnica:** A implementação de uma interface gráfica web utilizando Gradio (`gr.ChatInterface`) visa primariamente melhorar a acessibilidade e a usabilidade do sistema. Enquanto a interação via código ou console é adequada para desenvolvimento, uma GUI torna o assistente utilizável por pessoas sem conhecimentos técnicos. Facilita a demonstração das capacidades do sistema, oferece uma visualização clara do diálogo e permite funcionalidades como o "flagging" de respostas para recolha de feedback.

7.  **Implementação de Timeouts e Retries para APIs Externas:**
    *   **Justificação Técnica:** A integração com APIs externas (Mistral e Gemini) introduz dependências de rede e da disponibilidade desses serviços. Para aumentar a robustez do sistema, foram implementados mecanismos de timeout (ex: `GEMINI_TIMEOUT` de 30 segundos para chamadas à API Gemini, gerido com `threading`) e retries (ex: 3 tentativas para `get_embedding` e `generate_gemini_response`). Estas medidas previnem que a aplicação fique bloqueada indefinidamente ou falhe por completo devido a problemas temporários de conectividade ou sobrecarga nas APIs, garantindo uma experiência mais fiável para o utilizador.
