{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2u3B_oQGyCTo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df491bd-f287-422c-df2e-ac43a4aca9c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu --quiet\n",
        "!pip install google-generativeai --quiet\n",
        "!pip install requests --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_He4ZW7eqWME"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import faiss\n",
        "import requests\n",
        "import time\n",
        "import threading\n",
        "import tracemalloc\n",
        "import google.generativeai as genai\n",
        "\n",
        "\n",
        "from typing import List, Dict, Any\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KND3TyLKqd9V"
      },
      "outputs": [],
      "source": [
        "# Configurações globais\n",
        "MISTRAL_API_KEY = userdata.get('MISTRAL_KEY')\n",
        "GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "EMBEDDINGS_DIR = \"/content/drive/MyDrive/Colab Notebooks/projeto II V3/scrapyFiles/faiss_index\"\n",
        "BASE_WAIT_TIME = 2.0\n",
        "GEMINI_TIMEOUT = 30.0  # Timeout for Gemini API in seconds\n",
        "GEMINI_MAX_RETRIES = 3\n",
        "MAX_HISTORY_LENGTH = 10  # Limite de trocas no histórico de conversa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-6HbeyOqw1q"
      },
      "outputs": [],
      "source": [
        "# Inicializar o Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GDNJt92BHjx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "dc9cae47-cc1d-474a-ed93-65f6817c0c5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testando conexão com a API do Gemini...\n",
            "Teste bem-sucedido: A descoberta do Brasil é tradicionalmente atribuída a Pedro Álvares Cabral, em 22 de abril de 1500. ...\n"
          ]
        }
      ],
      "source": [
        "# Testar a API do Gemini\n",
        "print(\"Testando conexão com a API do Gemini...\")\n",
        "try:\n",
        "    test_model = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
        "    test_response = test_model.generate_content(\"Quem descobriu o Brasil?\")\n",
        "    print(f\"Teste bem-sucedido: {test_response.text[:100]}...\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro ao testar a API do Gemini: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWK8iHHmq_1b"
      },
      "outputs": [],
      "source": [
        "def measure_execution(func):\n",
        "    \"\"\"Decorator para medir tempo de execução e uso de memória.\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start_time = time.time()\n",
        "        tracemalloc.start()\n",
        "        result = func(*args, **kwargs)\n",
        "        current, peak = tracemalloc.get_traced_memory()\n",
        "        end_time = time.time()\n",
        "        metrics = {\n",
        "            \"execution_time\": end_time - start_time,\n",
        "            \"memory_used\": current / 10**6,\n",
        "            \"memory_peak\": peak / 10**6,\n",
        "        }\n",
        "        print(f\"Tempo de execução: {metrics['execution_time']:.2f} segundos\")\n",
        "        print(f\"Memória usada: {metrics['memory_used']:.2f} MB\")\n",
        "        print(f\"Memória máxima: {metrics['memory_peak']:.2f} MB\")\n",
        "        tracemalloc.stop()\n",
        "        return result, metrics\n",
        "    return wrapper\n",
        "\n",
        "def initialize_retriever(embeddings_dir: str):\n",
        "    \"\"\"Inicializa o sistema de retrieval.\"\"\"\n",
        "    print(\"Carregando recursos de retrieval...\")\n",
        "    try:\n",
        "        embeddings = np.load(os.path.join(embeddings_dir, \"book_embeddings.npy\"))\n",
        "        print(\"Embeddings carregados.\")\n",
        "        with open(os.path.join(embeddings_dir, \"book_metadata.pkl\"), \"rb\") as f:\n",
        "            metadata = pickle.load(f)\n",
        "        print(\"Metadados carregados.\")\n",
        "        index = faiss.read_index(os.path.join(embeddings_dir, \"book_index.faiss\"))\n",
        "        print(f\"Recursos carregados: {len(metadata)} livros indexados\")\n",
        "        return embeddings, metadata, index\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar recursos: {str(e)}\")\n",
        "        raise Exception(f\"Falha na inicialização: {str(e)}\")\n",
        "\n",
        "def get_embedding(text: str) -> np.ndarray:\n",
        "    \"\"\"Gera embedding usando a API Mistral.\"\"\"\n",
        "    print(f\"Gerando embedding para: {text}\")\n",
        "    headers = {\"Authorization\": f\"Bearer {MISTRAL_API_KEY}\", \"Content-Type\": \"application/json\"}\n",
        "    url = \"https://api.mistral.ai/v1/embeddings\"\n",
        "    payload = {\"model\": \"mistral-embed\", \"input\": text}\n",
        "\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            response = requests.post(url, json=payload, headers=headers, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                print(\"Embedding gerado com sucesso.\")\n",
        "                return np.array(response.json()[\"data\"][0][\"embedding\"]).astype('float32').reshape(1, -1)\n",
        "            elif response.status_code == 429:\n",
        "                wait_time = BASE_WAIT_TIME * (2 ** attempt)\n",
        "                print(f\"Rate limit excedido. Aguardando {wait_time:.2f} segundos...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                print(f\"Erro na API Mistral: {response.status_code} - {response.text}\")\n",
        "                raise Exception(f\"Erro na API Mistral: {response.status_code} - {response.text}\")\n",
        "        except requests.Timeout:\n",
        "            print(f\"Tempo limite excedido na tentativa {attempt + 1}.\")\n",
        "            time.sleep(BASE_WAIT_TIME)\n",
        "        except Exception as e:\n",
        "            print(f\"Exceção na tentativa {attempt + 1}: {str(e)}\")\n",
        "            time.sleep(BASE_WAIT_TIME)\n",
        "    raise Exception(\"Falha ao gerar embedding após 3 tentativas.\")\n",
        "\n",
        "def search(query: str, index, metadata, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Busca livros similares.\"\"\"\n",
        "    print(f\"Iniciando busca para: {query}\")\n",
        "    try:\n",
        "        query_embedding = get_embedding(query)\n",
        "        distances, indices = index.search(query_embedding, top_k)\n",
        "        print(\"Busca concluída no índice FAISS.\")\n",
        "        results = [\n",
        "            {\n",
        "                \"id\": metadata[idx][\"id\"],\n",
        "                \"titulo\": metadata[idx][\"titulo\"],\n",
        "                \"autor\": metadata[idx][\"autor\"],\n",
        "                \"relevance_score\": float(1.0 / (1.0 + dist)),\n",
        "                \"index\": int(idx)\n",
        "            }\n",
        "            for idx, dist in zip(indices[0], distances[0])\n",
        "        ]\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"Erro na busca: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def format_context(results: List[Dict[str, Any]], metadata: List[Dict[str, Any]]) -> str:\n",
        "    \"\"\"Formata os resultados para o Gemini.\"\"\"\n",
        "    print(\"Formatando contexto dos resultados.\")\n",
        "    context = \"Livros relevantes encontrados:\\n\\n\"\n",
        "    for i, result in enumerate(results):\n",
        "        book_meta = metadata[result[\"index\"]]\n",
        "        context += f\"**Livro {i+1}**\\n\"\n",
        "        context += f\"Título: {book_meta['titulo']}\\n\"\n",
        "        context += f\"Autor: {book_meta['autor']}\\n\"\n",
        "        if book_meta.get('co-autor'):\n",
        "            context += f\"Co-autor: {book_meta['co-autor']}\\n\"\n",
        "        context += f\"Idioma: {book_meta['idioma']}\\n\"\n",
        "        if book_meta.get('nome_comum'):\n",
        "            context += f\"Assuntos: {', '.join(book_meta['nome_comum'])}\\n\"\n",
        "        if book_meta.get('pais'):\n",
        "            context += f\"País: {book_meta['pais']}\\n\"\n",
        "        if book_meta.get('analitico'):\n",
        "            context += f\"Analítico: {book_meta['analitico']}\\n\"\n",
        "        if book_meta.get('itype'):\n",
        "            context += f\"Tipo: {book_meta['itype']}\\n\"\n",
        "        context += f\"Localização: {book_meta['shelvingloc']}\\n\"\n",
        "        context += f\"Cota: {book_meta['call_no']}\\n\\n\"\n",
        "    return context\n",
        "\n",
        "def format_history(history: List[Dict[str, str]]) -> str:\n",
        "    \"\"\"Formata o histórico de conversa para inclusão no prompt.\"\"\"\n",
        "    if not history:\n",
        "        return \"\"\n",
        "    history_text = \"Histórico de conversa:\\n\"\n",
        "    for i, entry in enumerate(history):\n",
        "        history_text += f\"**Pergunta {i+1}**: {entry['query']}\\n\"\n",
        "        history_text += f\"**Resposta {i+1}**: {entry['response']}\\n\\n\"\n",
        "    return history_text\n",
        "\n",
        "def generate_gemini_response(prompt: str) -> str:\n",
        "    \"\"\"Gera resposta do Gemini com retry e timeout.\"\"\"\n",
        "    model = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
        "    for attempt in range(GEMINI_MAX_RETRIES):\n",
        "        try:\n",
        "            result = [None]\n",
        "            def run_generate():\n",
        "                try:\n",
        "                    response = model.generate_content(prompt)\n",
        "                    result[0] = response.text\n",
        "                except Exception as e:\n",
        "                    result[0] = f\"Erro interno no Gemini: {str(e)}\"\n",
        "\n",
        "            thread = threading.Thread(target=run_generate)\n",
        "            thread.start()\n",
        "            thread.join(timeout=GEMINI_TIMEOUT)\n",
        "\n",
        "            if thread.is_alive():\n",
        "                print(f\"Tempo limite excedido na tentativa {attempt + 1}.\")\n",
        "                if attempt < GEMINI_MAX_RETRIES - 1:\n",
        "                    wait_time = BASE_WAIT_TIME * (2 ** attempt)\n",
        "                    print(f\"Aguardando {wait_time:.2f} segundos antes de tentar novamente...\")\n",
        "                    time.sleep(wait_time)\n",
        "                    continue\n",
        "                return \"Erro: Tempo limite excedido após várias tentativas.\"\n",
        "            if result[0] is None:\n",
        "                print(f\"Falha silenciosa na tentativa {attempt + 1}.\")\n",
        "                if attempt < GEMINI_MAX_RETRIES - 1:\n",
        "                    time.sleep(BASE_WAIT_TIME)\n",
        "                    continue\n",
        "                return \"Erro: Falha ao gerar resposta após várias tentativas.\"\n",
        "            if result[0].startswith(\"Erro interno no Gemini\"):\n",
        "                print(f\"Erro na tentativa {attempt + 1}: {result[0]}\")\n",
        "                if attempt < GEMINI_MAX_RETRIES - 1:\n",
        "                    time.sleep(BASE_WAIT_TIME)\n",
        "                    continue\n",
        "                return result[0]\n",
        "            print(\"Resposta gerada com sucesso.\")\n",
        "            return result[0]\n",
        "        except Exception as e:\n",
        "            print(f\"Erro na tentativa {attempt + 1}: {str(e)}\")\n",
        "            if attempt < GEMINI_MAX_RETRIES - 1:\n",
        "                time.sleep(BASE_WAIT_TIME)\n",
        "            continue\n",
        "    return \"Erro: Não foi possível gerar resposta após várias tentativas.\"\n",
        "\n",
        "@measure_execution\n",
        "def answer_question(query: str, embeddings, metadata, index, history: List[Dict[str, str]], top_k: int = 5) -> str:\n",
        "    \"\"\"Gera resposta com base na busca, incluindo histórico de conversa.\"\"\"\n",
        "    print(f\"Processando pergunta: {query}\")\n",
        "    try:\n",
        "        search_results = search(query, index, metadata, top_k=top_k)\n",
        "        if not search_results:\n",
        "            print(\"Nenhum resultado encontrado.\")\n",
        "            response = \"Não encontrei livros relevantes para sua consulta.\"\n",
        "        else:\n",
        "            context = format_context(search_results, metadata)\n",
        "            print(\"Contexto formatado, gerando resposta com Gemini.\")\n",
        "            print(f\"Prompt enviado ao Gemini (primeiros 500 caracteres):\\n{context[:500]}...\")\n",
        "            history_text = format_history(history)\n",
        "            prompt = f\"\"\"\n",
        "            Você é um assistente de biblioteca amigável. Responda à pergunta do usuário com base no contexto abaixo e no histórico de conversa, se relevante.\n",
        "            Se não houver informação suficiente, seja honesto e sugira algo útil.\n",
        "            Sempre refira a localização do livro(a biblioteca onde se econtra).\n",
        "\n",
        "            **HISTÓRICO:**\n",
        "            {history_text}\n",
        "\n",
        "            **CONTEXTO:**\n",
        "            {context}\n",
        "\n",
        "            **PERGUNTA:**\n",
        "            {query}\n",
        "\n",
        "            **RESPOSTA:**\n",
        "            \"\"\"\n",
        "            response = generate_gemini_response(prompt)\n",
        "\n",
        "        # Atualizar histórico\n",
        "        history.append({\"query\": query, \"response\": response})\n",
        "        if len(history) > MAX_HISTORY_LENGTH:\n",
        "            history.pop(0)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao gerar resposta: {str(e)}\")\n",
        "        response = f\"Ocorreu um erro: {str(e)}. Tente novamente ou reformule sua pergunta.\"\n",
        "        history.append({\"query\": query, \"response\": response})\n",
        "        if len(history) > MAX_HISTORY_LENGTH:\n",
        "            history.pop(0)\n",
        "        return response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perguntar_ao_assistente(embeddings, metadata, index):\n",
        "    \"\"\"Permite fazer perguntas ao assistente diretamente no console com histórico.\"\"\"\n",
        "    print(\"Bem-vindo ao Assistente de Biblioteca! Digite sua pergunta ou 'sair' para encerrar.\")\n",
        "    history = []\n",
        "    while True:\n",
        "        print(\"\\nHistórico de conversa:\")\n",
        "        for i, entry in enumerate(history):\n",
        "            print(f\"Pergunta {i+1}: {entry['query']}\")\n",
        "            print(f\"Resposta {i+1}: {entry['response'][:100]}...\")\n",
        "        pergunta = input(\"\\nDigite sua pergunta: \")\n",
        "        if pergunta.lower() == 'sair':\n",
        "            print(\"Encerrando o assistente.\")\n",
        "            break\n",
        "        response, metrics = answer_question(pergunta, embeddings, metadata, index, history)\n",
        "        print(f\"Resposta: {response}\")\n"
      ],
      "metadata": {
        "id": "1NFCLH95Q0sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --quiet"
      ],
      "metadata": {
        "id": "ijff_ROczIPP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "952bdeb5-6720-4682-c5ff-b32de3e5ebf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import time"
      ],
      "metadata": {
        "id": "mJ03CdQbm6Ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings, metadata, index = initialize_retriever(EMBEDDINGS_DIR)\n",
        "\n",
        "\n",
        "def chatbot_interface(embeddings, metadata, index):\n",
        "    \"\"\"Cria uma interface Gradio mínima para o assistente de biblioteca.\"\"\"\n",
        "    history = []\n",
        "\n",
        "    def respond(message, chat_history):\n",
        "        # Chame a função do seu chatbot para gerar a resposta\n",
        "        response, metrics = answer_question(message, embeddings, metadata, index, history)\n",
        "\n",
        "        # Simule um atraso para dar a impressão de que o chatbot está \"pensando\"\n",
        "        for i in range(len(response)):\n",
        "            time.sleep(0.01)\n",
        "            yield response[: i + 1]\n",
        "\n",
        "    demo = gr.ChatInterface(\n",
        "        respond,  # Use a função modificada aqui\n",
        "        type='messages',\n",
        "        flagging_mode=\"manual\",\n",
        "        flagging_options=[\"Like\", \"Spam\", \"Inappropriate\", \"Other\"],\n",
        "        save_history=True,\n",
        "    )\n",
        "    return demo # Return the demo object\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo = chatbot_interface(embeddings, metadata, index) # Call the function and assign the result to demo\n",
        "    demo.launch(debug = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mgI3a9CXqGTk",
        "outputId": "e2750d17-c5d4-4431-e858-01e8f8558e50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando recursos de retrieval...\n",
            "Embeddings carregados.\n",
            "Metadados carregados.\n",
            "Recursos carregados: 18266 livros indexados\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://4c5d18569c76a58112.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4c5d18569c76a58112.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processando pergunta: Ola. Quero saber mais sobre mineracao\n",
            "Iniciando busca para: Ola. Quero saber mais sobre mineracao\n",
            "Gerando embedding para: Ola. Quero saber mais sobre mineracao\n",
            "Embedding gerado com sucesso.\n",
            "Busca concluída no índice FAISS.\n",
            "Formatando contexto dos resultados.\n",
            "Contexto formatado, gerando resposta com Gemini.\n",
            "Prompt enviado ao Gemini (primeiros 500 caracteres):\n",
            "Livros relevantes encontrados:\n",
            "\n",
            "**Livro 1**\n",
            "Título: Conceitos de material grosseiro, mineral alterável e reserva mineral do solo : breves notas / António Augusto Guerra Réfega\n",
            "Autor: Réfega, António Augusto Guerra\n",
            "Idioma: Português.\n",
            "Assuntos: Ciência e gestäo do solo\n",
            "Localização: Escola Superior Agrária\n",
            "Cota: P30-1366\n",
            "\n",
            "**Livro 2**\n",
            "Título: Produçäo e exploraçäo : aspectos tecnológicos / Albino de Carvalho\n",
            "Autor: Carvalho, Albino de\n",
            "Idioma: Português.\n",
            "Assuntos: Produçäo florestal - Engenharia flor...\n",
            "Resposta gerada com sucesso.\n",
            "Tempo de execução: 2.93 segundos\n",
            "Memória usada: 0.09 MB\n",
            "Memória máxima: 0.36 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# EXECUÇÃO\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        embeddings, metadata, index = initialize_retriever(EMBEDDINGS_DIR)\n",
        "        print(\"Escolha o modo de interação:\")\n",
        "        print(\"1. Interface Gradio\")\n",
        "        print(\"2. Console (perguntar_ao_assistente)\")\n",
        "        choice = input(\"Digite 1 ou 2: \")\n",
        "        if choice == \"1\":\n",
        "            demo = chatbot_interface(embeddings, metadata, index)\n",
        "            try:\n",
        "                demo.launch(share=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao iniciar o Gradio: {str(e)}. Tente o modo console ou reinicie o runtime.\")\n",
        "        elif choice == \"2\":\n",
        "            perguntar_ao_assistente(embeddings, metadata, index)\n",
        "        else:\n",
        "            print(\"Escolha inválida. Encerrando.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao iniciar o sistema: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W3iYVcohMdIy",
        "outputId": "253b1244-b40b-429d-c920-71e5450ea440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando recursos de retrieval...\n",
            "Embeddings carregados.\n",
            "Metadados carregados.\n",
            "Recursos carregados: 1883 livros indexados\n",
            "Escolha o modo de interação:\n",
            "1. Interface Gradio\n",
            "2. Console (perguntar_ao_assistente)\n",
            "Digite 1 ou 2: 2\n",
            "Bem-vindo ao Assistente de Biblioteca! Digite sua pergunta ou 'sair' para encerrar.\n",
            "\n",
            "Histórico de conversa:\n",
            "\n",
            "Digite sua pergunta: Eu quero livros de mineracao\n",
            "Processando pergunta: Eu quero livros de mineracao\n",
            "Iniciando busca para: Eu quero livros de mineracao\n",
            "Gerando embedding para: Eu quero livros de mineracao\n",
            "Embedding gerado com sucesso.\n",
            "Busca concluída no índice FAISS.\n",
            "Formatando contexto dos resultados.\n",
            "Contexto formatado, gerando resposta com Gemini.\n",
            "Prompt enviado ao Gemini (primeiros 500 caracteres):\n",
            "Livros relevantes encontrados:\n",
            "\n",
            "**Livro 1**\n",
            "Título: Ensino da geologia : perspectivas científicas / Miguel Telles Antunes\n",
            "Autor: Antunes, Miguel Telles\n",
            "Idioma: Português.\n",
            "Assuntos: Geologia - ensino das ciências da terra - formação de professores em exercício\n",
            "Localização: Escola Superior de Educação\n",
            "Cota: 551.1 ANT Ens\n",
            "\n",
            "**Livro 2**\n",
            "Título: Minerals in soil environments / ed. J.M. Bigham... [et al.]\n",
            "Autor: Bigham, J.M.\n",
            "Idioma: Inglês.\n",
            "Assuntos: Fertilidade do solo\n",
            "Localização: Escola Superior Agr...\n",
            "Resposta gerada com sucesso.\n",
            "Tempo de execução: 5.92 segundos\n",
            "Memória usada: 0.06 MB\n",
            "Memória máxima: 0.15 MB\n",
            "Resposta: Olá! Com base na sua pergunta, encontrei alguns livros que podem ser do seu interesse:\n",
            "\n",
            "*   **Minerals in soil environments / ed. J.M. Bigham... [et al.]** de J.M. Bigham, localizado na **Escola Superior Agrária**, com a cota **P35-23823**. Este livro aborda a fertilidade do solo, que está relacionada com a presença de minerais.\n",
            "*   **Mineralogia dos solos de Säo Tomé e Príncipe / J. Bailim Pissarra, J. Carvalho Cardoso, J. Sacadura Garcia** de J. Bailim Pissarra, também na **Escola Superior Agrária**, com a cota **P32-2927**. Este livro foca na mineralogia dos solos de São Tomé e Príncipe, abordando a classificação e génese do solo.\n",
            "\n",
            "Embora não haja livros com o título exato de \"mineração\", estes podem fornecer informações relevantes sobre minerais em contextos específicos.\n",
            "\n",
            "Histórico de conversa:\n",
            "Pergunta 1: Eu quero livros de mineracao\n",
            "Resposta 1: Olá! Com base na sua pergunta, encontrei alguns livros que podem ser do seu interesse:\n",
            "\n",
            "*   **Minera...\n",
            "\n",
            "Digite sua pergunta: sair\n",
            "Encerrando o assistente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_interface(embeddings, metadata, index):\n",
        "    \"\"\"Cria uma interface Gradio mínima para o assistente de biblioteca.\"\"\"\n",
        "    history = []\n",
        "\n",
        "    def respond(message, chat_history):\n",
        "        if not message.strip():\n",
        "            chat_history.append([message, \"Por favor, digite uma pergunta.\"])\n",
        "            return chat_history\n",
        "        try:\n",
        "            response, metrics = answer_question(message, embeddings, metadata, index, history)\n",
        "            history.append({\"query\": message, \"response\": response})\n",
        "            if len(history) > MAX_HISTORY_LENGTH:\n",
        "                history.pop(0)\n",
        "            chat_history.append([message, f\"{response}\\n\\nMétricas: Tempo={metrics['execution_time']:.2f}s, Memória={metrics['memory_used']:.2f}MB\"])\n",
        "            return chat_history\n",
        "        except Exception as e:\n",
        "            print(f\"Erro: {str(e)}\")\n",
        "            chat_history.append([message, f\"Erro: {str(e)}\"])\n",
        "            return chat_history\n",
        "\n",
        "    iface = gr.Interface(\n",
        "        fn=respond,\n",
        "        inputs=[\n",
        "            gr.Textbox(placeholder=\"Ex.: Livros sobre biodiversidade florestal\", label=\"Pergunta\"),\n",
        "            gr.Chatbot(label=\"Conversa\")\n",
        "        ],\n",
        "        outputs=[\n",
        "            gr.Chatbot(label=\"Conversa\")\n",
        "        ],\n",
        "        title=\"Assistente de Biblioteca\",\n",
        "        description=\"Pesquise livros na biblioteca acadêmica.\",\n",
        "        allow_flagging=\"never\"\n",
        "    )\n",
        "    return iface"
      ],
      "metadata": {
        "id": "XkhFZm7UtHTf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}