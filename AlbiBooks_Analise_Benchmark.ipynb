```python
# AlbiBooks Fase II - Notebook de Análise de Resultados do Benchmark

## Objetivo
Este notebook é destinado a carregar, processar e analisar os resultados gerados pelo notebook de benchmarking `AlbiBooks_FaseII_Benchmark.ipynb`. Ele fornece ferramentas para calcular métricas de desempenho quantitativas, preparar os dados para avaliação qualitativa e visualizar os resultados.

## Pré-requisitos
1.  **Ficheiro de Resultados do Benchmark:** Um ficheiro JSON Lines (e.g., `benchmark_fase_ii_resultados.jsonl` ou `simulated_benchmark_fase_ii_resultados.jsonl`) gerado pelo `AlbiBooks_FaseII_Benchmark.ipynb`.
2.  **Bibliotecas Python:** Todas as bibliotecas listadas na primeira célula de código devem estar instaladas (pandas, matplotlib, seaborn, numpy, json).

## Funcionalidades Principais
- **Carregamento de Dados:**
    - A função `carregar_resultados(filepath)` lê o ficheiro JSON Lines e converte-o num DataFrame Pandas.
- **Análise de Métricas de Desempenho Quantitativas:**
    - Extrai as métricas de tempo de execução, memória usada e pico de memória da coluna `metricas_desempenho`.
    - Calcula estatísticas descritivas (média, mediana, min, max, desvio padrão) para estas métricas.
    - Gera visualizações como histogramas e boxplots para ilustrar a distribuição do tempo de resposta e, opcionalmente, do uso de memória.
- **Preparação para Análise Qualitativa:**
    - Adiciona colunas vazias (ou com valores padrão) ao DataFrame para que o utilizador possa posteriormente anotar manualmente a qualidade das respostas. As colunas sugeridas incluem:
        - `avaliacao_relevancia_resposta`
        - `avaliacao_precisao_resposta`
        - `avaliacao_completude_resposta`
        - `avaliacao_relevancia_docs_recuperados`
        - `observacoes_qualitativas`
    - O utilizador pode exportar este DataFrame para CSV/Excel, preencher estas colunas, e depois reimportá-lo para análises qualitativas mais profundas.
- **Análise de Documentos Recuperados:**
    - Calcula o número médio de documentos recuperados por pergunta.
    - Permite a análise dos `relevance_score` dos documentos recuperados (se disponíveis e relevantes para a métrica de distância usada).
- **Inspeção Detalhada de Casos Específicos:**
    - A função `mostrar_detalhes_pergunta(df, id_pergunta_ou_indice)` permite visualizar de forma formatada todos os dados recolhidos para uma pergunta específica (texto da pergunta, resposta, contexto enviado ao LLM, documentos recuperados, métricas), facilitando a análise aprofundada de casos de interesse.

## Como Utilizar
1.  Certifique-se de que o ficheiro de resultados do benchmark (e.g., `benchmark_fase_ii_resultados.jsonl`) está no caminho correto, conforme especificado na variável `resultados_benchmark_filepath`.
2.  Execute as células do notebook sequencialmente.
3.  Analise as estatísticas e gráficos gerados para as métricas de desempenho.
4.  Para a análise qualitativa:
    *   Pode exportar o DataFrame (após a célula "Preparação para Análise Qualitativa") para um formato como CSV.
    *   Preencha as colunas de avaliação manualmente nesse ficheiro.
    *   Reimporte o ficheiro CSV para o notebook para realizar análises sobre as avaliações (e.g., calcular médias de scores qualitativos, gerar gráficos de distribuição das avaliações).
5.  Utilize a função `mostrar_detalhes_pergunta` para investigar respostas ou comportamentos específicos.

Este notebook serve como um ponto de partida para a análise e pode ser expandido com visualizações e análises mais complexas conforme necessário.

# 1. Célula de Instalações e Importações
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import json
import os # Para caminhos de ficheiros

# Configurações para os gráficos
sns.set_theme(style="whitegrid")
plt.rcParams['figure.figsize'] = (10, 6)

# 2. Célula de Carregamento de Dados

# Tenta usar o diretório do Google Drive primeiro, depois um subdiretório local, depois o diretório atual.
RESULTS_DIR_OPTIONS = [
    '/content/drive/MyDrive/AlbiBooksIndex/',
    './AlbiBooksIndex/',
    './'
]
RESULTS_DIR = None
for r_dir in RESULTS_DIR_OPTIONS:
    if os.path.exists(r_dir):
        RESULTS_DIR = r_dir
        print(f"Usando directório de resultados: {RESULTS_DIR}")
        break
if RESULTS_DIR is None:
    RESULTS_DIR = './' # Fallback final se nenhum existir (improvável se o notebook estiver em algum lugar)
    print(f"AVISO: Nenhum dos directórios de resultados padrão foi encontrado. Usando directório local actual: {RESULTS_DIR}")


# MODIFICAÇÃO AQUI: Apontar para o ficheiro de resultados simulados
resultados_benchmark_filepath = os.path.join(RESULTS_DIR, "simulated_benchmark_fase_ii_resultados.jsonl")

def carregar_resultados(filepath: str) -> pd.DataFrame | None:
    """
    Lê um ficheiro JSON Lines e converte os dados para um DataFrame do Pandas.
    """
    data = []
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                data.append(json.loads(line))
        print(f"Carregados {len(data)} registos de '{filepath}'.")
        return pd.DataFrame(data)
    except FileNotFoundError:
        print(f"ERRO: Ficheiro de resultados '{filepath}' não encontrado.")
        print(f"Localização tentada: {os.path.abspath(filepath)}")
        print("Certifique-se de que executou o notebook 'AlbiBooks_FaseII_Benchmark.ipynb' (com simulação) primeiro e que o caminho está correto.")
        return None
    except json.JSONDecodeError:
        print(f"ERRO: Erro ao descodificar JSON no ficheiro '{filepath}'. Verifique o formato do ficheiro.")
        return None
    except Exception as e:
        print(f"Ocorreu um erro inesperado ao carregar os resultados: {e}")
        return None

# Carregar os dados
df_resultados = carregar_resultados(resultados_benchmark_filepath)

if df_resultados is not None:
    print("\nPrimeiras 5 linhas do DataFrame (Resultados Simulados):")
    print(df_resultados.head())
    print("\nInformações do DataFrame (Resultados Simulados):")
    df_resultados.info()
else:
    print("\nNão foi possível carregar os dados simulados. As análises subsequentes podem falhar ou não ser executadas.")
    # Criar um DataFrame vazio com as colunas esperadas para evitar erros nas células seguintes
    df_resultados = pd.DataFrame(columns=[
        'id_pergunta', 'pergunta_texto', 'resposta_gerada',
        'documentos_recuperados', 'contexto_para_gemini', 'metricas_desempenho'
    ])


# 3. Célula de Análise de Métricas de Desempenho Quantitativas

if df_resultados is not None and not df_resultados.empty and 'metricas_desempenho' in df_resultados.columns:
    print("\n--- Análise de Métricas de Desempenho Quantitativas ---")

    df_resultados['tempo_execucao_s'] = df_resultados['metricas_desempenho'].apply(
        lambda x: x.get('execution_time_seconds') if isinstance(x, dict) else None
    )
    df_resultados['memoria_usada_mb'] = df_resultados['metricas_desempenho'].apply(
        lambda x: x.get('memory_used_MB') if isinstance(x, dict) else None
    )
    df_resultados['pico_memoria_mb'] = df_resultados['metricas_desempenho'].apply(
        lambda x: x.get('memory_peak_MB') if isinstance(x, dict) else None
    )

    df_resultados['tempo_execucao_s'] = pd.to_numeric(df_resultados['tempo_execucao_s'], errors='coerce')
    df_resultados['memoria_usada_mb'] = pd.to_numeric(df_resultados['memoria_usada_mb'], errors='coerce')
    df_resultados['pico_memoria_mb'] = pd.to_numeric(df_resultados['pico_memoria_mb'], errors='coerce')

    print("\nEstatísticas Descritivas do Desempenho:")
    desc_perf = df_resultados[['tempo_execucao_s', 'memoria_usada_mb', 'pico_memoria_mb']].describe()
    print(desc_perf)

    if not df_resultados['tempo_execucao_s'].dropna().empty:
        plt.figure()
        sns.histplot(df_resultados['tempo_execucao_s'].dropna(), kde=True)
        plt.title('Histograma do Tempo de Execução por Pergunta (Simulado)')
        plt.xlabel('Tempo de Execução (segundos)')
        plt.ylabel('Frequência')
        plt.show()

        plt.figure()
        sns.boxplot(x=df_resultados['tempo_execucao_s'].dropna())
        plt.title('Boxplot do Tempo de Execução por Pergunta (Simulado)')
        plt.xlabel('Tempo de Execução (segundos)')
        plt.show()
    else:
        print("Não há dados de tempo de execução para gerar gráficos.")

    if not df_resultados['pico_memoria_mb'].dropna().empty:
        plt.figure()
        sns.histplot(df_resultados['pico_memoria_mb'].dropna(), kde=False)
        plt.title('Histograma do Pico de Memória Usada (Simulado)')
        plt.xlabel('Pico de Memória (MB)')
        plt.ylabel('Frequência')
        plt.show()
    else:
        print("\nNão há dados suficientes de pico de memória para gerar gráficos.")
else:
    print("DataFrame de resultados vazio, não carregado, ou sem 'metricas_desempenho'. A saltar análise de desempenho.")

# 4. Célula de Preparação para Análise Qualitativa

if df_resultados is not None: # Mesmo que esteja vazio, podemos adicionar colunas
    print("\n--- Preparação para Análise Qualitativa ---")
    colunas_qualitativas = [
        'avaliacao_relevancia_resposta', 'avaliacao_precisao_resposta',
        'avaliacao_completude_resposta', 'avaliacao_relevancia_docs_recuperados',
        'docs_recuperados_relevantes_ids', 'observacoes_qualitativas'
    ]
    for col in colunas_qualitativas:
        if col not in df_resultados.columns: df_resultados[col] = pd.NA
    df_resultados['observacoes_qualitativas'] = df_resultados['observacoes_qualitativas'].astype(str).fillna("") # Assegurar que é string
    print("Colunas adicionadas/asseguradas para anotação manual:")
    print(df_resultados[['id_pergunta'] + colunas_qualitativas].head())
else:
    print("DataFrame de resultados não carregado. A saltar preparação para análise qualitativa.")

# 5. Célula para Análise de Documentos Recuperados (Básica)

if df_resultados is not None and not df_resultados.empty and 'documentos_recuperados' in df_resultados.columns:
    print("\n--- Análise de Documentos Recuperados (Simulado) ---")
    df_resultados['num_docs_recuperados'] = df_resultados['documentos_recuperados'].apply(lambda x: len(x) if isinstance(x, list) else 0)
    print("\nEstatísticas do Número de Documentos Recuperados por Pergunta:")
    print(df_resultados['num_docs_recuperados'].describe())

    def get_first_doc_score(docs_list):
        if isinstance(docs_list, list) and len(docs_list) > 0 and isinstance(docs_list[0], dict):
            return docs_list[0].get('relevance_score')
        return np.nan
    df_resultados['score_primeiro_doc'] = df_resultados['documentos_recuperados'].apply(get_first_doc_score)

    scores_primeiro_doc_validos = df_resultados['score_primeiro_doc'].dropna()
    if not scores_primeiro_doc_validos.empty:
        print("\nEstatísticas do Score de Relevância do Primeiro Documento Recuperado:")
        print(scores_primeiro_doc_validos.describe())
        plt.figure(); sns.histplot(scores_primeiro_doc_validos, kde=True, bins=min(5, len(scores_primeiro_doc_validos.unique())));
        plt.title('Histograma do Score de Relevância do Primeiro Documento (Simulado)'); plt.xlabel('Score de Relevância'); plt.ylabel('Frequência'); plt.show()
    else:
        print("Não foram encontrados scores de relevância para o primeiro documento.")

    def get_avg_doc_score(docs_list):
        if isinstance(docs_list, list) and len(docs_list) > 0:
            scores = [d.get('relevance_score') for d in docs_list if isinstance(d, dict) and d.get('relevance_score') is not None]
            return np.mean(scores) if scores else np.nan
        return np.nan
    df_resultados['score_medio_docs_recuperados'] = df_resultados['documentos_recuperados'].apply(get_avg_doc_score)

    scores_medios_validos = df_resultados['score_medio_docs_recuperados'].dropna()
    if not scores_medios_validos.empty:
        print("\nEstatísticas do Score Médio de Relevância dos Documentos Recuperados:")
        print(scores_medios_validos.describe())
    else:
        print("Não foi possível calcular o score médio dos documentos recuperados.")
else:
    print("DataFrame de resultados vazio ou coluna 'documentos_recuperados' em falta. A saltar análise de documentos.")

# 6. Célula de Exemplo para Inspeção de Casos Específicos

def mostrar_detalhes_pergunta(df: pd.DataFrame, id_pergunta: str = None, indice_linha: int = None):
    if df is None or df.empty: print("DataFrame de resultados está vazio."); return
    item_series = None
    if id_pergunta:
        linha_filtrada = df[df['id_pergunta'] == id_pergunta]
        if not linha_filtrada.empty: item_series = linha_filtrada.iloc[0]
    elif indice_linha is not None:
        if 0 <= indice_linha < len(df): item_series = df.iloc[indice_linha]
        else: print(f"ERRO: Índice de linha {indice_linha} fora dos limites."); return
    else: print("ERRO: Fornecer 'id_pergunta' ou 'indice_linha'."); return

    if item_series is None: print("Nenhum resultado encontrado."); return

    print(f"\n--- Detalhes para Pergunta ID: {item_series.get('id_pergunta', 'N/A')} ---")
    print(f"Texto da Pergunta: {item_series.get('pergunta_texto', 'N/A')}")
    print("-" * 30); print(f"Resposta Gerada:\n{item_series.get('resposta_gerada', 'N/A')}"); print("-" * 30)
    contexto = item_series.get('contexto_para_gemini', 'N/A'); print(f"Contexto Enviado (primeiros 300 caracteres):\n{str(contexto)[:300]}..."); print("-" * 30)
    print("Documentos Recuperados:")
    docs = item_series.get('documentos_recuperados', [])
    if isinstance(docs, list) and docs:
        for i, doc in enumerate(docs):
            if isinstance(doc, dict): print(f"  Doc {i+1}: ID {doc.get('id')}, Título: \"{doc.get('titulo')}\", Autor: {doc.get('autor')}, Score: {doc.get('relevance_score')}")
    else: print("  Nenhum documento ou formato inesperado.")
    print("-" * 30); print("Métricas de Desempenho:"); metricas = item_series.get('metricas_desempenho', {})
    if isinstance(metricas, dict):
        for k, v in metricas.items(): print(f"  {k}: {v}")
    print("-" * 50)

if df_resultados is not None and not df_resultados.empty:
    print("\n--- Exemplo de Inspeção de Caso Específico (Simulado) ---")
    mostrar_detalhes_pergunta(df_resultados, indice_linha=0)
    if len(df_resultados) > 1:
         mostrar_detalhes_pergunta(df_resultados, indice_linha=min(1, len(df_resultados)-1))
else:
    print("DataFrame de resultados vazio. A saltar exemplo de inspeção.")

```
