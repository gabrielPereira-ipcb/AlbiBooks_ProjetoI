{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2u3B_oQGyCTo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddfb1994-1573-469c-d8be-11f7b108743b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu --quiet\n",
        "!pip install google-generativeai --quiet\n",
        "!pip install requests --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_He4ZW7eqWME"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import faiss\n",
        "import requests\n",
        "import time\n",
        "import threading\n",
        "import tracemalloc\n",
        "import google.generativeai as genai\n",
        "\n",
        "\n",
        "from typing import List, Dict, Any\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShtduKfBcZWR",
        "outputId": "cee2e3c6-b564-4cf0-a03c-938e59d118fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KND3TyLKqd9V"
      },
      "outputs": [],
      "source": [
        "# Configurações globais\n",
        "\n",
        "MISTRAL_API_KEY = userdata.get('MISTRAL_KEY')\n",
        "GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "EMBEDDINGS_DIR = \"/content/drive/MyDrive/Colab Notebooks/projeto II V3/scrapyFiles/faiss_index\"\n",
        "BASE_WAIT_TIME = 2.0\n",
        "GEMINI_TIMEOUT = 30.0  # Timeout for Gemini API in seconds\n",
        "GEMINI_MAX_RETRIES = 3\n",
        "MAX_HISTORY_LENGTH = 10  # Limite de trocas no histórico de conversa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-6HbeyOqw1q"
      },
      "outputs": [],
      "source": [
        "# Inicializar o Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWK8iHHmq_1b"
      },
      "outputs": [],
      "source": [
        "def measure_execution(func):\n",
        "    \"\"\"Decorator para medir tempo de execução e uso de memória.\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start_time = time.time()\n",
        "        tracemalloc.start()\n",
        "        result = func(*args, **kwargs)\n",
        "        current, peak = tracemalloc.get_traced_memory()\n",
        "        end_time = time.time()\n",
        "        metrics = {\n",
        "            \"execution_time\": end_time - start_time,\n",
        "            \"memory_used\": current / 10**6,\n",
        "            \"memory_peak\": peak / 10**6,\n",
        "        }\n",
        "        print(f\"Tempo de execução: {metrics['execution_time']:.2f} segundos\")\n",
        "        print(f\"Memória usada: {metrics['memory_used']:.2f} MB\")\n",
        "        print(f\"Memória máxima: {metrics['memory_peak']:.2f} MB\")\n",
        "        tracemalloc.stop()\n",
        "        return result, metrics\n",
        "    return wrapper\n",
        "\n",
        "def initialize_retriever(embeddings_dir: str):\n",
        "    \"\"\"Inicializa o sistema de retrieval.\"\"\"\n",
        "    print(\"Carregando recursos de retrieval...\")\n",
        "    try:\n",
        "        embeddings = np.load(os.path.join(embeddings_dir, \"book_embeddings.npy\"))\n",
        "        print(\"Embeddings carregados.\")\n",
        "        with open(os.path.join(embeddings_dir, \"book_metadata.pkl\"), \"rb\") as f:\n",
        "            metadata = pickle.load(f)\n",
        "        print(\"Metadados carregados.\")\n",
        "        index = faiss.read_index(os.path.join(embeddings_dir, \"book_index.faiss\"))\n",
        "        print(f\"Recursos carregados: {len(metadata)} livros indexados\")\n",
        "        return embeddings, metadata, index\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar recursos: {str(e)}\")\n",
        "        raise Exception(f\"Falha na inicialização: {str(e)}\")\n",
        "\n",
        "def get_embedding(text: str) -> np.ndarray:\n",
        "    \"\"\"Gera embedding usando a API Mistral.\"\"\"\n",
        "    print(f\"Gerando embedding para: {text}\")\n",
        "    headers = {\"Authorization\": f\"Bearer {MISTRAL_API_KEY}\", \"Content-Type\": \"application/json\"}\n",
        "    url = \"https://api.mistral.ai/v1/embeddings\"\n",
        "    payload = {\"model\": \"mistral-embed\", \"input\": text}\n",
        "\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            response = requests.post(url, json=payload, headers=headers, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                print(\"Embedding gerado com sucesso.\")\n",
        "                return np.array(response.json()[\"data\"][0][\"embedding\"]).astype('float32').reshape(1, -1)\n",
        "            elif response.status_code == 429:\n",
        "                wait_time = BASE_WAIT_TIME * (2 ** attempt)\n",
        "                print(f\"Rate limit excedido. Aguardando {wait_time:.2f} segundos...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                print(f\"Erro na API Mistral: {response.status_code} - {response.text}\")\n",
        "                raise Exception(f\"Erro na API Mistral: {response.status_code} - {response.text}\")\n",
        "        except requests.Timeout:\n",
        "            print(f\"Tempo limite excedido na tentativa {attempt + 1}.\")\n",
        "            time.sleep(BASE_WAIT_TIME)\n",
        "        except Exception as e:\n",
        "            print(f\"Exceção na tentativa {attempt + 1}: {str(e)}\")\n",
        "            time.sleep(BASE_WAIT_TIME)\n",
        "    raise Exception(\"Falha ao gerar embedding após 3 tentativas.\")\n",
        "\n",
        "def search(query: str, index, metadata, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Busca livros similares.\"\"\"\n",
        "    print(f\"Iniciando busca para: {query}\")\n",
        "    try:\n",
        "        query_embedding = get_embedding(query)\n",
        "        distances, indices = index.search(query_embedding, top_k)\n",
        "        print(\"Busca concluída no índice FAISS.\")\n",
        "        results = [\n",
        "            {\n",
        "                \"id\": metadata[idx][\"id\"],\n",
        "                \"titulo\": metadata[idx][\"titulo\"],\n",
        "                \"autor\": metadata[idx][\"autor\"],\n",
        "                \"relevance_score\": float(1.0 / (1.0 + dist)),\n",
        "                \"index\": int(idx)\n",
        "            }\n",
        "            for idx, dist in zip(indices[0], distances[0])\n",
        "        ]\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"Erro na busca: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def format_context(results: List[Dict[str, Any]], metadata: List[Dict[str, Any]]) -> str:\n",
        "    \"\"\"Formata os resultados para o Gemini.\"\"\"\n",
        "    print(\"Formatando contexto dos resultados.\")\n",
        "    context = \"Livros relevantes encontrados:\\n\\n\"\n",
        "    for i, result in enumerate(results):\n",
        "        book_meta = metadata[result[\"index\"]]\n",
        "        context += f\"**Livro {i+1}**\\n\"\n",
        "        context += f\"Título: {book_meta['titulo']}\\n\"\n",
        "        context += f\"Autor: {book_meta['autor']}\\n\"\n",
        "        if book_meta.get('co-autor'):\n",
        "            context += f\"Co-autor: {book_meta['co-autor']}\\n\"\n",
        "        context += f\"Idioma: {book_meta['idioma']}\\n\"\n",
        "        if book_meta.get('nome_comum'):\n",
        "            context += f\"Assuntos: {', '.join(book_meta['nome_comum'])}\\n\"\n",
        "        if book_meta.get('pais'):\n",
        "            context += f\"País: {book_meta['pais']}\\n\"\n",
        "        if book_meta.get('analitico'):\n",
        "            context += f\"Analítico: {book_meta['analitico']}\\n\"\n",
        "        if book_meta.get('itype'):\n",
        "            context += f\"Tipo: {book_meta['itype']}\\n\"\n",
        "        context += f\"Localização: {book_meta['shelvingloc']}\\n\"\n",
        "        context += f\"Cota: {book_meta['call_no']}\\n\\n\"\n",
        "    return context\n",
        "\n",
        "def format_history(history: List[Dict[str, str]]) -> str:\n",
        "    \"\"\"Formata o histórico de conversa para inclusão no prompt.\"\"\"\n",
        "    if not history:\n",
        "        return \"\"\n",
        "    history_text = \"Histórico de conversa:\\n\"\n",
        "    for i, entry in enumerate(history):\n",
        "        history_text += f\"**Pergunta {i+1}**: {entry['query']}\\n\"\n",
        "        history_text += f\"**Resposta {i+1}**: {entry['response']}\\n\\n\"\n",
        "    return history_text\n",
        "\n",
        "def generate_gemini_response(prompt: str) -> str:\n",
        "    \"\"\"Gera resposta do Gemini com retry e timeout.\"\"\"\n",
        "    model = genai.GenerativeModel('gemini-2.5-flash-preview-05-20')\n",
        "    for attempt in range(GEMINI_MAX_RETRIES):\n",
        "        try:\n",
        "            result = [None]\n",
        "            def run_generate():\n",
        "                try:\n",
        "                    response = model.generate_content(prompt)\n",
        "                    result[0] = response.text\n",
        "                except Exception as e:\n",
        "                    result[0] = f\"Erro interno no Gemini: {str(e)}\"\n",
        "\n",
        "            thread = threading.Thread(target=run_generate)\n",
        "            thread.start()\n",
        "            thread.join(timeout=GEMINI_TIMEOUT)\n",
        "\n",
        "            if thread.is_alive():\n",
        "                print(f\"Tempo limite excedido na tentativa {attempt + 1}.\")\n",
        "                if attempt < GEMINI_MAX_RETRIES - 1:\n",
        "                    wait_time = BASE_WAIT_TIME * (2 ** attempt)\n",
        "                    print(f\"Aguardando {wait_time:.2f} segundos antes de tentar novamente...\")\n",
        "                    time.sleep(wait_time)\n",
        "                    continue\n",
        "                return \"Erro: Tempo limite excedido após várias tentativas.\"\n",
        "            if result[0] is None:\n",
        "                print(f\"Falha silenciosa na tentativa {attempt + 1}.\")\n",
        "                if attempt < GEMINI_MAX_RETRIES - 1:\n",
        "                    time.sleep(BASE_WAIT_TIME)\n",
        "                    continue\n",
        "                return \"Erro: Falha ao gerar resposta após várias tentativas.\"\n",
        "            if result[0].startswith(\"Erro interno no Gemini\"):\n",
        "                print(f\"Erro na tentativa {attempt + 1}: {result[0]}\")\n",
        "                if attempt < GEMINI_MAX_RETRIES - 1:\n",
        "                    time.sleep(BASE_WAIT_TIME)\n",
        "                    continue\n",
        "                return result[0]\n",
        "            print(\"Resposta gerada com sucesso.\")\n",
        "            return result[0]\n",
        "        except Exception as e:\n",
        "            print(f\"Erro na tentativa {attempt + 1}: {str(e)}\")\n",
        "            if attempt < GEMINI_MAX_RETRIES - 1:\n",
        "                time.sleep(BASE_WAIT_TIME)\n",
        "            continue\n",
        "    return \"Erro: Não foi possível gerar resposta após várias tentativas.\"\n",
        "\n",
        "@measure_execution\n",
        "def answer_question(query: str, embeddings, metadata, index, history: List[Dict[str, str]], top_k: int = 10) -> str:\n",
        "    \"\"\"Gera resposta com base na busca, incluindo histórico de conversa.\"\"\"\n",
        "    print(f\"Processando pergunta: {query}\")\n",
        "    try:\n",
        "        search_results = search(query, index, metadata, top_k=top_k)\n",
        "        if not search_results:\n",
        "            print(\"Nenhum resultado encontrado.\")\n",
        "            response = \"Não encontrei livros relevantes para sua consulta.\"\n",
        "        else:\n",
        "            context = format_context(search_results, metadata)\n",
        "            print(\"Contexto formatado, gerando resposta com Gemini.\")\n",
        "            print(f\"Prompt enviado ao Gemini (primeiros 500 caracteres):\\n{context[:500]}...\")\n",
        "            history_text = format_history(history)\n",
        "            prompt = f\"\"\"\n",
        "            Você é um assistente de biblioteca amigável. Responda à pergunta do usuário com base no contexto abaixo e no histórico de conversa, se relevante.\n",
        "            Se não houver informação suficiente, seja honesto e sugira algo útil.\n",
        "            Sempre refira a localização do livro(a biblioteca onde se econtra).\n",
        "\n",
        "            **HISTÓRICO:**\n",
        "            {history_text}\n",
        "\n",
        "            **CONTEXTO:**\n",
        "            {context}\n",
        "\n",
        "            **PERGUNTA:**\n",
        "            {query}\n",
        "\n",
        "            **RESPOSTA:**\n",
        "            \"\"\"\n",
        "            response = generate_gemini_response(prompt)\n",
        "\n",
        "        # Atualizar histórico\n",
        "        history.append({\"query\": query, \"response\": response})\n",
        "        if len(history) > MAX_HISTORY_LENGTH:\n",
        "            history.pop(0)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao gerar resposta: {str(e)}\")\n",
        "        response = f\"Ocorreu um erro: {str(e)}. Tente novamente ou reformule sua pergunta.\"\n",
        "        history.append({\"query\": query, \"response\": response})\n",
        "        if len(history) > MAX_HISTORY_LENGTH:\n",
        "            history.pop(0)\n",
        "        return response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perguntar_ao_assistente(embeddings, metadata, index):\n",
        "    \"\"\"Permite fazer perguntas ao assistente diretamente no console com histórico.\"\"\"\n",
        "    print(\"Bem-vindo ao Assistente de Biblioteca! Digite sua pergunta ou 'sair' para encerrar.\")\n",
        "    history = []\n",
        "    while True:\n",
        "        print(\"\\nHistórico de conversa:\")\n",
        "        for i, entry in enumerate(history):\n",
        "            print(f\"Pergunta {i+1}: {entry['query']}\")\n",
        "            print(f\"Resposta {i+1}: {entry['response'][:100]}...\")\n",
        "        pergunta = input(\"\\nDigite sua pergunta: \")\n",
        "        if pergunta.lower() == 'sair':\n",
        "            print(\"Encerrando o assistente.\")\n",
        "            break\n",
        "        response, metrics = answer_question(pergunta, embeddings, metadata, index, history)\n",
        "        print(f\"Resposta: {response}\")\n"
      ],
      "metadata": {
        "id": "1NFCLH95Q0sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --quiet"
      ],
      "metadata": {
        "id": "ijff_ROczIPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import time"
      ],
      "metadata": {
        "id": "mJ03CdQbm6Ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings, metadata, index = initialize_retriever(EMBEDDINGS_DIR)\n",
        "\n",
        "\n",
        "def chatbot_interface(embeddings, metadata, index):\n",
        "    \"\"\"Cria uma interface Gradio mínima para o assistente de biblioteca.\"\"\"\n",
        "    history = []\n",
        "\n",
        "    def respond(message, chat_history):\n",
        "        # Chame a função do seu chatbot para gerar a resposta\n",
        "        response, metrics = answer_question(message, embeddings, metadata, index, history)\n",
        "\n",
        "        # Simule um atraso para dar a impressão de que o chatbot está \"pensando\"\n",
        "        for i in range(len(response)):\n",
        "            time.sleep(0.01)\n",
        "            yield response[: i + 1]\n",
        "\n",
        "    demo = gr.ChatInterface(\n",
        "        respond,  # Use a função modificada aqui\n",
        "        type='messages',\n",
        "        flagging_mode=\"manual\",\n",
        "        flagging_options=[\"Like\", \"Spam\", \"Inappropriate\", \"Other\"],\n",
        "        save_history=True,\n",
        "    )\n",
        "    return demo # Return the demo object\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo = chatbot_interface(embeddings, metadata, index) # Call the function and assign the result to demo\n",
        "    demo.launch(debug = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mgI3a9CXqGTk",
        "outputId": "f88b86e2-614c-4a12-98fc-0162f10cf2a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando recursos de retrieval...\n",
            "Embeddings carregados.\n",
            "Metadados carregados.\n",
            "Recursos carregados: 18266 livros indexados\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://25bf6c3da003ae9491.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://25bf6c3da003ae9491.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processando pergunta: Olá. Que livros de programacao ha na biblioteca?\n",
            "Iniciando busca para: Olá. Que livros de programacao ha na biblioteca?\n",
            "Gerando embedding para: Olá. Que livros de programacao ha na biblioteca?\n",
            "Embedding gerado com sucesso.\n",
            "Busca concluída no índice FAISS.\n",
            "Formatando contexto dos resultados.\n",
            "Contexto formatado, gerando resposta com Gemini.\n",
            "Prompt enviado ao Gemini (primeiros 500 caracteres):\n",
            "Livros relevantes encontrados:\n",
            "\n",
            "**Livro 1**\n",
            "Título: Como programar em educação especial / Jesús Garrido Landivar\n",
            "Autor: Landivar, Jesús Garrido\n",
            "Idioma: Português.\n",
            "Assuntos: Educação especial\n",
            "Localização: Biblioteca Central\n",
            "Cota: 376 - 152004\n",
            "\n",
            "**Livro 2**\n",
            "Título: Introdução à programação usando o Pascal / J. Pavão Martins\n",
            "Autor: Martins, J. Pavão\n",
            "Idioma: Português.\n",
            "Assuntos: Métodos matemáticos e estatísticos, Programa de computador, Programação de computador - Pascal\n",
            "Localização: Escola Superior...\n",
            "Resposta gerada com sucesso.\n",
            "Tempo de execução: 10.37 segundos\n",
            "Memória usada: 0.09 MB\n",
            "Memória máxima: 0.29 MB\n",
            "Processando pergunta: Qual foi a ultima pergunta que eu fiz?\n",
            "Iniciando busca para: Qual foi a ultima pergunta que eu fiz?\n",
            "Gerando embedding para: Qual foi a ultima pergunta que eu fiz?\n",
            "Embedding gerado com sucesso.\n",
            "Busca concluída no índice FAISS.\n",
            "Formatando contexto dos resultados.\n",
            "Contexto formatado, gerando resposta com Gemini.\n",
            "Prompt enviado ao Gemini (primeiros 500 caracteres):\n",
            "Livros relevantes encontrados:\n",
            "\n",
            "**Livro 1**\n",
            "Título: Como perguntar : teoria e prática da construção de perguntas em entrevistas e questionários / William Foddy ; trad. Luís Campos\n",
            "Autor: Foody, William\n",
            "Idioma: Português.\n",
            "Assuntos: Sociologia - Ténica de entrevista - Inquérito\n",
            "Localização: Escola Superior Agrária\n",
            "Cota: U70-24370\n",
            "\n",
            "**Livro 2**\n",
            "Título: Que resposta dá quando lhe perguntam como funciona um computador\n",
            "Autor: Neigenfind, Rulf\n",
            "Idioma: Português.\n",
            "Assuntos: Atitudes perante o computador\n",
            "L...\n",
            "Resposta gerada com sucesso.\n",
            "Tempo de execução: 2.25 segundos\n",
            "Memória usada: 0.06 MB\n",
            "Memória máxima: 0.30 MB\n",
            "Processando pergunta: E quais livros voce recomendou?\n",
            "Iniciando busca para: E quais livros voce recomendou?\n",
            "Gerando embedding para: E quais livros voce recomendou?\n",
            "Embedding gerado com sucesso.\n",
            "Busca concluída no índice FAISS.\n",
            "Formatando contexto dos resultados.\n",
            "Contexto formatado, gerando resposta com Gemini.\n",
            "Prompt enviado ao Gemini (primeiros 500 caracteres):\n",
            "Livros relevantes encontrados:\n",
            "\n",
            "**Livro 1**\n",
            "Título: O livro\n",
            "Autor: Ricketts, Michael\n",
            "Idioma: Português.\n",
            "Assuntos: Literatura infantil\n",
            "Localização: Escola Superior de Educação\n",
            "Cota: 087.5-96 RIC Liv\n",
            "\n",
            "**Livro 2**\n",
            "Título: O livro da selva / ed. Girassol Ediçöes\n",
            "Autor: Girassol Ediçöes\n",
            "Idioma: Português.\n",
            "Assuntos: Literatura infantil\n",
            "Localização: Escola Superior de Educação\n",
            "Cota: 087.5-34 Liv\n",
            "\n",
            "**Livro 3**\n",
            "Título: Quase que os vi viver / Vitorino Nemésio\n",
            "Autor: Nemésio, Vitorino, 1901\n",
            "Idioma: Portugu...\n",
            "Resposta gerada com sucesso.\n",
            "Tempo de execução: 5.96 segundos\n",
            "Memória usada: 0.07 MB\n",
            "Memória máxima: 0.31 MB\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://25bf6c3da003ae9491.gradio.live\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}