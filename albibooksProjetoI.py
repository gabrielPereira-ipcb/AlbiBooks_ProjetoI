# -*- coding: utf-8 -*-
"""FINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WQHgazYx-Ni8ipTzxdGs8e06JRYWgG4x
"""

# Instalação das bibliotecas necessárias
!pip install -q torch transformers transformers accelerate bitsandbytes langchain sentence-transformers faiss-cpu openpyxl pacmap datasets langchain-community ragatouille

# Carregar o dataset
import datasets
ds = datasets.load_dataset("Eitanli/goodreads", split="train")
ds = ds.select(range(500))  # Selecionar as primeiras 500 linhas
print(ds[0])  # Exibir a primeira linha para verificar

from langchain.docstore.document import Document as LangchainDocument
from langchain.text_splitter import RecursiveCharacterTextSplitter
from tqdm.notebook import tqdm

# Criar a base de conhecimento
RAW_KNOWLEDGE_BASE = [
    LangchainDocument(page_content=str(doc["Description"]), metadata={"Author": doc["Author"], "Book": doc["Book"], "Genres": doc["Genres"]}) for doc in tqdm(ds)
]

# Processar documentos em chunks
MARKDOWN_SEPARATORS = ["\\n#{1,6} ", "```\\n", "\\n\\*\\*\\*+\\n", "\\n---+\\n", "\\n___+\\n", "\\n\\n", "\\n", " ", ""]
text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50, separators=MARKDOWN_SEPARATORS)
docs_processed = []
for doc in RAW_KNOWLEDGE_BASE:
    docs_processed += text_splitter.split_documents([doc])
print(f"Número de documentos processados: {len(docs_processed)}")

from langchain.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores.utils import DistanceStrategy

# Função para criar índice vetorial com modelo de embeddings especificado
def create_vector_database(docs, model_name):
    embedding_model = HuggingFaceEmbeddings(
        model_name=model_name,
        multi_process=True,
        model_kwargs={"device": "cuda"},
        encode_kwargs={"normalize_embeddings": True},
    )
    return FAISS.from_documents(docs, embedding_model, distance_strategy=DistanceStrategy.COSINE)

# Criar índice vetorial com modelo padrão
EMBEDDING_MODEL_NAME = "thenlper/gte-small"
KNOWLEDGE_VECTOR_DATABASE = create_vector_database(docs_processed, EMBEDDING_MODEL_NAME)

!pip install huggingface-cli

!huggingface-cli login

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch

def load_llm_with_prompt(model_name):
    # Configuração do modelo em 4 bits
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
    )
    # Carregar modelo e tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Definir o formato do prompt no estilo chat
    # Instead of using apply_chat_template, manually format the prompt
    def format_prompt(context, question):
        prompt = f"""Using the information contained in the context,
give a comprehensive answer to the question.
Respond only to the question asked, response should be concise and relevant to the question.
Provide the number of the source document when relevant.
If the answer cannot be deduced from the context, do not give an answer.

Context:
{context}
---
Now here is the question you need to answer.

Question: {question}"""
        return prompt

    return model, tokenizer, format_prompt # Return the formatting function

import time
import tracemalloc
import json

# Função para coletar métricas (tempo e memória)
def measure_execution(func):
    def wrapper(*args, **kwargs):
        # Iniciar medição de tempo e memória
        start_time = time.time()
        tracemalloc.start()

        # Executar a função
        result = func(*args, **kwargs)

        # Parar medição
        current, peak = tracemalloc.get_traced_memory()
        end_time = time.time()

        # Retornar métricas juntamente com o resultado
        metrics = {
            "execution_time": end_time - start_time,
            "memory_used": current / 10**6,
            "memory_peak": peak / 10**6,
        }
        # Exibir métricas
        print(f"Tempo de execução: {end_time - start_time:.2f} segundos")
        print(f"Memória usada: {current / 10**6:.2f} MB")
        print(f"Memória máxima: {peak / 10**6:.2f} MB")

        tracemalloc.stop()
        return result, metrics

    return wrapper

# Atualizando a função para incluir medições

@measure_execution
def answer_with_rag(
    question: str,
    llm,
    knowledge_index,
    RAG_PROMPT_TEMPLATE, # Add RAG_PROMPT_TEMPLATE as an argument
    num_retrieved_docs: int = 10,
    num_docs_final: int = 3,
):
    print("=> Recuperando documentos...")
    relevant_docs = knowledge_index.similarity_search(
        query=question, k=num_retrieved_docs
    )
    relevant_docs = [doc.page_content for doc in relevant_docs]  # Manter apenas o texto

    relevant_docs = relevant_docs[:num_docs_final]

    # Construir o prompt final
    context = "\nExtracted documents:\n"
    context += "".join(
        [f"Document {str(i)}:::\n" + doc for i, doc in enumerate(relevant_docs)]
    )

    # Call the RAG_PROMPT_TEMPLATE function to format the prompt
    prompt = RAG_PROMPT_TEMPLATE(question=question, context=context) # Call the function to get the formatted string

    print("=> Gerando resposta...")
    answer = llm(prompt)[0]["generated_text"]

    return answer, relevant_docs

# Função para fazer perguntas com base em um arquivo JSON
def questions_from_file(llm, knowledge_index, RAG_PROMPT_TEMPLATE, file_path="questions.json"): # Add RAG_PROMPT_TEMPLATE as an argument
    try:
        with open(file_path, "r") as file:
            questions = json.load(file)
    except FileNotFoundError:
        print(f"Arquivo {file_path} não encontrado. Certifique-se de que o arquivo existe.")
        return

    answers = []

    for i, question in enumerate(questions):
        print(f"\nPergunta {i+1}: {question}")
        # Pass RAG_PROMPT_TEMPLATE to answer_with_rag
        (answer, relevant_docs), metrics = answer_with_rag(question, llm, knowledge_index, RAG_PROMPT_TEMPLATE)

        # Adicionar métricas ao resultado
        answers.append({
            "question": question,
            "answer": answer,
            "docs": relevant_docs,
            "metrics": metrics
        })

        print(f"Resposta: {answer[: ]}...")
        print("Documentos relevantes:")
        for j, doc in enumerate(relevant_docs):
            print(f"Documento {j}: {doc[:200]}...")

    # Salvar respostas e métricas em um arquivo JSON
    with open("answers.json", "w") as outfile:
        json.dump(answers, outfile, indent=4)
    print("Respostas e métricas salvas em 'answers.json'.")

# Exemplo de uso
# Crie um arquivo JSON chamado 'questions.json' com perguntas
# Depois, execute:
#questions_from_file(READER_LLM, KNOWLEDGE_VECTOR_DATABASE)

# Lista de modelos para testar no benchmark
model_names = ["HuggingFaceH4/zephyr-7b-beta", "meta-llama/Llama-2-13b-chat-hf","distilgpt2"]

# Executar o benchmark para cada modelo
for model_name in model_names:
    print(f"\n=== Testando o modelo: {model_name} ===")

    # Carregar o modelo atual
    model, tokenizer, RAG_PROMPT_TEMPLATE = load_llm_with_prompt(model_name)

    # Criar pipeline com o modelo carregado
    READER_LLM = pipeline(
        model=model,
        tokenizer=tokenizer,
        task="text-generation",
        do_sample=True,
        temperature=0.2,
        max_new_tokens=500,
    )

    # Executar as perguntas do arquivo JSON
    # Pass RAG_PROMPT_TEMPLATE to questions_from_file
    questions_from_file(READER_LLM, KNOWLEDGE_VECTOR_DATABASE, RAG_PROMPT_TEMPLATE, file_path="questions.json")

